{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Components\n",
    "The objective of this notebook is to create a pipeline for preprocessing and traning of a model. \n",
    "\n",
    "    + The prepprocessing is a separate component that is run on a sepcific node\n",
    "    + The train component is also executing on a separate node.\n",
    "\n",
    "## Important for using Copilot\n",
    "Copilot disrupts the internal features such as intelliscence feautres, .e.g., autocompletion. To avoide its suggestion automatically, from the setting disable automatic inline suggestion.\n",
    "Then, you can get the suggestion by alt + \\ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall azure-ai-ml\n",
    "!pip install azure-ai-ml\n",
    "\n",
    "!pip show azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to workspace\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Get a handler to the workspace and Datastore\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farbodtaymouri1\n",
      "farbodtaymouri2\n",
      "azureml_globaldatasets\n",
      "workspaceworkingdirectory\n",
      "workspaceartifactstore\n",
      "workspacefilestore\n",
      "workspaceblobstore\n",
      "AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu\n",
      "AzureML-ACPT-pytorch-1.12-py38-cuda11.6-gpu\n",
      "AzureML-ACPT-pytorch-1.12-py39-cuda11.6-gpu\n",
      "AzureML-ACPT-pytorch-1.11-py38-cuda11.5-gpu\n",
      "AzureML-ACPT-pytorch-1.11-py38-cuda11.3-gpu\n",
      "AzureML-responsibleai-0.21-ubuntu20.04-py38-cpu\n",
      "AzureML-responsibleai-0.20-ubuntu20.04-py38-cpu\n",
      "AzureML-tensorflow-2.5-ubuntu20.04-py38-cuda11-gpu\n",
      "AzureML-tensorflow-2.6-ubuntu20.04-py38-cuda11-gpu\n",
      "AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu\n",
      "AzureML-sklearn-1.0-ubuntu20.04-py38-cpu\n",
      "AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu\n",
      "AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu\n",
      "AzureML-pytorch-1.8-ubuntu18.04-py37-cuda11-gpu\n",
      "AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n",
      "AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu\n",
      "AzureML-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu\n",
      "AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu\n",
      "AzureML-Triton\n",
      "AzureML-Designer-Score\n",
      "AzureML-VowpalWabbit-8.8.0\n",
      "AzureML-PyTorch-1.3-CPU\n"
     ]
    }
   ],
   "source": [
    "# Get the list of compute nodes and print them\n",
    "compute_list = ml_client.compute.list()\n",
    "for compute in compute_list:\n",
    "    print(compute.name)\n",
    "\n",
    "# get the list of datasets and print them\n",
    "dataset_list = ml_client.datastores.list()\n",
    "for dataset in dataset_list:\n",
    "    print(dataset.name)\n",
    "\n",
    "# Get the list of environment\n",
    "evs = ml_client.environments.list()\n",
    "for e in evs:\n",
    "    print(e.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farbodtaymouri1 Running\n",
      "farbodtaymouri2 Running\n"
     ]
    }
   ],
   "source": [
    "# Getting the status of compute nodes\n",
    "from azure.ai.ml.entities import ComputeInstance, AmlCompute\n",
    "\n",
    "# Creat a function from the above code that takes an input paramter for turning nodes on or off\n",
    "def compute_node_status(status):\n",
    "    \n",
    "    compute_list = ml_client.compute.list()\n",
    "    for compute in compute_list:\n",
    "        ci_name = compute.name\n",
    "        ci_basic_state = ml_client.compute.get(ci_name)\n",
    "        print(compute.name, ci_basic_state.state)\n",
    "\n",
    "        # Running if a node is stopped\n",
    "        if status == 'on':\n",
    "            if ci_basic_state.state == \"Stopped\":\n",
    "                ml_client.compute.begin_start(ci_name).wait()\n",
    "                print(ci_name, \"is starting\")\n",
    "        else:\n",
    "            if ci_basic_state.state == \"Running\":\n",
    "                ml_client.compute.begin_stop(ci_name).wait()\n",
    "                print(ci_name, \"is stopping\")\n",
    "\n",
    "\n",
    "\n",
    "compute_node_status(status = 'on')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create a folder for the script files\n",
    "script_folder = 'src'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/prep-data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/prep-data.py\n",
    "# import libraries\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mlflow\n",
    "\n",
    "def main(args):\n",
    "    # read data\n",
    "    df = get_data(args.input_data)\n",
    "\n",
    "    cleaned_data = clean_data(df)\n",
    "\n",
    "    normalized_data = normalize_data(cleaned_data)\n",
    "    print('args.output_data: ', args.output_data)\n",
    "\n",
    "    # See https://docs.python.org/3/library/pathlib.html#concrete-paths\n",
    "    output_df = normalized_data.to_csv((Path(args.output_data) / \"diabetes.csv\"), index = False)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Count the rows and print the result\n",
    "    row_count = (len(df))\n",
    "    print('Preparing {} rows of data'.format(row_count))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that removes missing values\n",
    "def clean_data(df):\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that normalizes the data\n",
    "def normalize_data(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--input_data\", dest='input_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--output_data\", dest='output_data',\n",
    "                        type=str)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Reading the data asset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m registered_data_asset  \u001b[39m=\u001b[39m ml_client\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mget(name \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdiabetes-local\u001b[39m\u001b[39m'\u001b[39m, version \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(registered_data_asset\u001b[39m.\u001b[39mpath)\n\u001b[1;32m      4\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Reading the data asset\n",
    "registered_data_asset  = ml_client.data.get(name ='diabetes-local', version = 1)\n",
    "df = pd.read_csv(registered_data_asset.path)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train-model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train-model.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import argparse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "\n",
    "from mlflow.pyfunc import PythonModel, PythonModelContext\n",
    "# https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-mlflow-models?view=azureml-api-2&tabs=wrapper#logging-custom-models\n",
    "class ModelWrapper(PythonModel):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def predict(self, context: PythonModelContext, data):\n",
    "        # You don't have to keep the semantic meaning of `predict`. You can use here model.recommend(), model.forecast(), etc\n",
    "        return self._model.predict_proba(data)\n",
    "\n",
    "    # You can even add extra functions if you need to. Since the model is serialized,\n",
    "    # all of them will be available when you load your model back.\n",
    "    def predict_batch(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # read data\n",
    "    df = get_data(args.training_data)\n",
    "\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # train model\n",
    "    model = train_model(args.n_estimators, args.max_depth, args.model_output, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # evaluate model\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# # function that reads the data\n",
    "# def get_data(path):\n",
    "#     print(\"Reading data...\")\n",
    "#     df = pd.read_csv(path)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(data_path):\n",
    "\n",
    "    # Read all csv files in the data directory\n",
    "    all_files = glob.glob(data_path + \"/*.csv\")\n",
    "    print('all_files in get-data(): ', all_files)\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that splits the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    X, y = df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness',\n",
    "    'SerumInsulin','BMI','DiabetesPedigree','Age']].values, df['Diabetic'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# function that trains the model\n",
    "def train_model(n_estimators, max_depth, model_output, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth = max_depth, n_estimators = n_estimators, random_state=0)\n",
    "    model = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "    # print(\"Training model...\")\n",
    "    # model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "    y_probs = model.predict_proba(X_test)\n",
    "\n",
    "    #Logging the model artifact\n",
    "    signature = infer_signature(X_test, y_probs)\n",
    "    mlflow.pyfunc.log_model(\"RFclassifier\", \n",
    "                            python_model=ModelWrapper(model),\n",
    "                            signature=signature)\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "# function that evaluates the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "    mlflow.log_metric(\"Accuracy\", acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\")\n",
    "    mlflow.log_artifact(\"ROC-Curve.png\")    \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--n_estimators\", dest='n_estimators',\n",
    "                        type=int, default=10)\n",
    "    parser.add_argument(\"--max_depth\", dest='max_depth',\n",
    "                        type=int, default=3)\n",
    "    parser.add_argument(\"--model_output\", dest='model_output',\n",
    "                        type=str)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Yaml file for the components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prep-data.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile prep-data.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: prep_data\n",
    "display_name: Prepare training data\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data: \n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ./src\n",
    "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
    "command: >-\n",
    "  python prep-data.py \n",
    "  --input_data ${{inputs.input_data}}\n",
    "  --output_data ${{outputs.output_data}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train-model.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile train-model.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: train_model\n",
    "display_name: Train a decision tree classifier model\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  training_data: \n",
    "    type: uri_folder\n",
    "  n_estimators:\n",
    "    type: integer\n",
    "    default: 5\n",
    "  max_depth:\n",
    "    type: integer\n",
    "    default: 3\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: mlflow_model\n",
    "code: ./src\n",
    "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
    "command: >-\n",
    "  python train-model.py \n",
    "  --training_data ${{inputs.training_data}} \n",
    "  --n_estimators  ${{inputs.n_estimators}}\n",
    "  --max_depth ${{inputs.max_depth}}\n",
    "  --model_output ${{outputs.model_output}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "parent_dir = \"\"\n",
    "\n",
    "prep_data = load_component(source=parent_dir + \"./prep-data.yml\")\n",
    "train_decision_tree = load_component(source=parent_dir + \"./train-model.yml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def diabetes_classification(pipeline_job_input):\n",
    "    clean_data = prep_data(input_data=pipeline_job_input)\n",
    "    clean_data.compute = 'farbodtaymouri1' # Assigning the component to a compute node\n",
    "\n",
    "    train_model = train_decision_tree(training_data=clean_data.outputs.output_data, n_estimators=10, max_depth=3)\n",
    "    train_model.compute = 'farbodtaymouri2'\n",
    "\n",
    "    return {\n",
    "        \"pipeline_job_transformed_data\": clean_data.outputs.output_data,\n",
    "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_job = diabetes_classification(Input(type=AssetTypes.URI_FILE, path=\"azureml:diabetes-local:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name: diabetes_classification\n",
      "type: pipeline\n",
      "inputs:\n",
      "  pipeline_job_input:\n",
      "    type: uri_file\n",
      "    path: azureml:diabetes-local:1\n",
      "outputs:\n",
      "  pipeline_job_transformed_data:\n",
      "    type: uri_folder\n",
      "  pipeline_job_trained_model:\n",
      "    type: mlflow_model\n",
      "jobs:\n",
      "  clean_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.inputs.pipeline_job_input}}\n",
      "    outputs:\n",
      "      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n",
      "    compute: azureml:farbodtaymouri1\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: prep_data\n",
      "      version: '1'\n",
      "      display_name: Prepare training data\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/farbodtaymouri1/code/my-azure-ml-projects/pipeline-component/src\n",
      "      is_deterministic: true\n",
      "  train_model:\n",
      "    type: command\n",
      "    inputs:\n",
      "      training_data:\n",
      "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
      "      n_estimators: 10\n",
      "      max_depth: 3\n",
      "    outputs:\n",
      "      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n",
      "    compute: azureml:farbodtaymouri2\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: train_model\n",
      "      version: '1'\n",
      "      display_name: Train a decision tree classifier model\n",
      "      type: command\n",
      "      inputs:\n",
      "        training_data:\n",
      "          type: uri_folder\n",
      "        n_estimators:\n",
      "          type: integer\n",
      "          default: '5'\n",
      "        max_depth:\n",
      "          type: integer\n",
      "          default: '3'\n",
      "      outputs:\n",
      "        model_output:\n",
      "          type: mlflow_model\n",
      "      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --n_estimators  ${{inputs.n_estimators}}\n",
      "        --max_depth ${{inputs.max_depth}} --model_output ${{outputs.model_output}} '\n",
      "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/farbodtaymouri1/code/my-azure-ml-projects/pipeline-component/src\n",
      "      is_deterministic: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the output mode (VERY IMPORTANT)\n",
    "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\n",
    "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\"\n",
    "# set pipeline level datastore\n",
    "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
    "\n",
    "# # print the pipeline job again to review the changes\n",
    "# print(pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_diabetes1</td><td>coral_guava_tdwg7lxjf2</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/coral_guava_tdwg7lxjf2?wsid=/subscriptions/2a21ade8-9d70-4d5a-a619-083b264d1d56/resourcegroups/mlcertificate1/workspaces/ft_ml&amp;tid=71f8feea-4caa-4230-a785-dca61147bceb\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f8e11f98d30>}, 'outputs': {'pipeline_job_transformed_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f8e11f99a50>, 'pipeline_job_trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f8e11f9a080>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/farbodtaymouri1/code/my-azure-ml-projects/pipeline-component', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8e11f9a1d0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'diabetes_classification', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_transformed_data': {}, 'pipeline_job_trained_model': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'clean_data': Command({'parameters': {}, 'init': False, 'name': 'clean_data', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/farbodtaymouri1/code/my-azure-ml-projects/pipeline-component', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8e11f99360>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': 'farbodtaymouri1', 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_data': '${{parent.outputs.pipeline_job_transformed_data}}'}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8e11f98c70>}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f8e11f992d0>}, 'component': 'azureml_anonymous:00c4a050-e467-453e-ad73-48830a9a1a59', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'aaee2748-4430-4139-8f53-ea3266e23172', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'train_model': Command({'parameters': {}, 'init': False, 'name': 'train_model', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/farbodtaymouri1/code/my-azure-ml-projects/pipeline-component', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f8e11f99840>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': 'farbodtaymouri2', 'services': None, 'comment': None, 'job_inputs': {'n_estimators': '10', 'max_depth': '3', 'training_data': '${{parent.jobs.clean_data.outputs.output_data}}'}, 'job_outputs': {'model_output': '${{parent.outputs.pipeline_job_trained_model}}'}, 'inputs': {'n_estimators': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8e11f9a050>, 'max_depth': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8e11f99420>, 'training_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f8e11f99480>}, 'outputs': {'model_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f8e11f99ba0>}, 'component': 'azureml_anonymous:f4e3356b-7cf2-4a2d-8038-23bb8289b6b4', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '5fcb8839-f876-42f5-8670-941b0acece30', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'coral_guava_tdwg7lxjf2', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/farbodtaymouri/my-azure-ml-projects.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'fc23336fa9df4618185f5dd588a2ca8af7e1acc5', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/2a21ade8-9d70-4d5a-a619-083b264d1d56/resourceGroups/mlcertificate1/providers/Microsoft.MachineLearningServices/workspaces/ft_ml/jobs/coral_guava_tdwg7lxjf2', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/farbodtaymouri1/code/my-azure-ml-projects/pipeline-component', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f8e11f99180>, 'serialize': <msrest.serialization.Serializer object at 0x7f8e11f99bd0>, 'display_name': 'diabetes_classification', 'experiment_name': 'pipeline_diabetes1', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://australiaeast.api.azureml.ms/mlflow/v1.0/subscriptions/2a21ade8-9d70-4d5a-a619-083b264d1d56/resourceGroups/mlcertificate1/providers/Microsoft.MachineLearningServices/workspaces/ft_ml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/coral_guava_tdwg7lxjf2?wsid=/subscriptions/2a21ade8-9d70-4d5a-a619-083b264d1d56/resourcegroups/mlcertificate1/workspaces/ft_ml&tid=71f8feea-4caa-4230-a785-dca61147bceb', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_diabetes1\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from azure.ai.ml import RecurrencePattern, RecurrenceTrigger, TimeZone, JobSchedule\n",
    "\n",
    "schedule_name = \"simple_sdk_create_schedule_recurrence\"\n",
    "\n",
    "schedule_start_time = datetime.datetime.utcnow()\n",
    "schedule_start_time = schedule_start_time + datetime.timedelta(minutes=2)\n",
    "\n",
    "recurrence_trigger = RecurrenceTrigger(\n",
    "    frequency=\"day\",\n",
    "    interval=1,\n",
    "    schedule=RecurrencePattern(hours=10, minutes=[0, 1]),\n",
    "    start_time=schedule_start_time,\n",
    "    time_zone=TimeZone.UTC,\n",
    ")\n",
    "\n",
    "job_schedule = JobSchedule(\n",
    "    name=schedule_name, trigger=recurrence_trigger, create_job=pipeline_job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_schedule = ml_client.schedules.begin_create_or_update(\n",
    "    schedule=job_schedule\n",
    ").result()\n",
    "print(job_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_node_status(status= 'off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
